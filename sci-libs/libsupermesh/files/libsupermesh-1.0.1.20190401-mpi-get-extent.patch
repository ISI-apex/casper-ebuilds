--- a/src/Parallel_Supermesh.F90	2020-10-05 08:26:03.391477743 -0000
+++ b/src/Parallel_Supermesh.F90	2020-10-05 08:27:18.219481516 -0000
@@ -203,12 +203,13 @@
       end subroutine pack_data_b
     end interface
 
-    integer :: buffer_size, dim, ele_b, i, ierr, int_extent, j, k, loc_b, &
-      & nelements_b, nelements_send, nnodes_send, node, position, real_extent
+    integer :: buffer_size, dim, ele_b, i, ierr, j, k, loc_b, &
+      & nelements_b, nelements_send, nnodes_send, node, position
     integer, dimension(:), allocatable :: elements_send, send_enlist, &
       & send_nodes_array
     integer(kind = c_int8_t), dimension(:), allocatable :: data
     real(kind = real_kind), dimension(:), allocatable :: send_positions
+    integer(kind = MPI_ADDRESS_KIND) int_lb, int_extent, real_lb, real_extent
     type(integer_map) :: node_map
     type(integer_set) :: nodes_send
 
@@ -220,8 +221,8 @@
     loc_b = size(enlist_b, 1)
     nelements_b = size(enlist_b, 2)
 
-    call MPI_Type_extent(mpi_real_kind, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
-    call MPI_Type_extent(MPI_INTEGER, int_extent, ierr);  assert(ierr == MPI_SUCCESS)
+    call MPI_Type_get_extent(mpi_real_kind, real_lb, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
+    call MPI_Type_get_extent(MPI_INTEGER, int_lb, int_extent, ierr);  assert(ierr == MPI_SUCCESS)
 
     allocate(elements_send(nelements_b))
     nsends = 0
--- a/./src/tests/test_parallel_p1_inner_product_2d.F90	2020-10-05 08:29:27.835488050 -0000
+++ b/./src/tests/test_parallel_p1_inner_product_2d.F90	2020-10-05 08:30:21.039490732 -0000
@@ -41,7 +41,8 @@
   real(kind = real_kind), parameter :: area_ref = 0.5_real_kind, &
     & integral_ref = 8.3333333333333398e-2_real_kind
 
-  integer :: ierr, integer_extent, nprocs, rank, real_extent
+  integer :: ierr, nprocs, rank
+  integer(kind = MPI_ADDRESS_KIND) :: integer_lb, integer_extent, real_lb, real_extent
  
   integer :: nelements_a, nelements_b, nnodes_a, nnodes_b
   integer, dimension(:), allocatable :: ele_owner_a, ele_owner_b
@@ -76,8 +77,8 @@
   write(nprocs_chr, "(i0)") nprocs
   rank_chr = adjustl(rank_chr)
   nprocs_chr = adjustl(nprocs_chr)
-  call MPI_Type_extent(MPI_INTEGER, integer_extent, ierr);  assert(ierr == MPI_SUCCESS)
-  call MPI_Type_extent(mpi_real_kind, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
+  call MPI_Type_get_extent(MPI_INTEGER, integer_lb, integer_extent, ierr);  assert(ierr == MPI_SUCCESS)
+  call MPI_Type_get_extent(mpi_real_kind, real_lb, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
 
   ! Read the donor mesh partition
   call read_node(trim(basename_a) // "_" // trim(nprocs_chr) // "_" // trim(rank_chr) // ".node", dim = 2, positions = positions_a)
--- a/./src/tests/test_parallel_supermesh_2d.F90	2020-10-05 08:31:24.383493926 -0000
+++ b/./src/tests/test_parallel_supermesh_2d.F90	2020-10-05 08:32:22.019496831 -0000
@@ -39,7 +39,8 @@
   implicit none
 
   integer :: ele_a, ele_b, ele_c, i, ierr, loc_b, n_tris_c, nelements_a, &
-    & nelements_b, nprocs, rank, real_extent
+    & nelements_b, nprocs, rank
+  integer(kind = MPI_ADDRESS_KIND) :: real_lb, real_extent
   integer, dimension(:), allocatable :: ele_owner_a, ele_owner_b
   integer, dimension(:, :), allocatable :: enlist_a, enlist_b
   real(kind = real_kind) :: area_parallel, area_serial, integral_parallel, &
@@ -62,7 +63,7 @@
   write(nprocs_chr, "(i0)") nprocs
   rank_chr = adjustl(rank_chr)
   nprocs_chr = adjustl(nprocs_chr)
-  call MPI_Type_extent(mpi_real_kind, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
+  call MPI_Type_get_extent(mpi_real_kind, real_lb, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
 
   if(rank == 0) then
     call read_node("data/square_0_01.node", dim = 2, positions = positions_a)
--- a/./src/tests/test_parallel_p2_inner_product_2d.F90	2020-10-05 08:36:18.559508756 -0000
+++ b/./src/tests/test_parallel_p2_inner_product_2d.F90	2020-10-05 08:37:12.059511453 -0000
@@ -45,7 +45,8 @@
   real(kind = real_kind), parameter :: area_ref = 0.5_real_kind, &
     & integral_ref = 2.7083333333333272e-02_real_kind
 
-  integer :: ierr, integer_extent, nprocs, rank, real_extent
+  integer :: ierr, nprocs, rank
+  integer(kind = MPI_ADDRESS_KIND) :: real_lb, real_extent, integer_lb, integer_extent
 
   integer :: nelements_a, nelements_b, nnodes_p1_a, nnodes_p2_a, &
     & nnodes_p1_b, nnodes_p2_b
@@ -82,8 +83,8 @@
   write(nprocs_chr, "(i0)") nprocs
   rank_chr = adjustl(rank_chr)
   nprocs_chr = adjustl(nprocs_chr)
-  call MPI_Type_extent(MPI_INTEGER, integer_extent, ierr);  assert(ierr == MPI_SUCCESS)
-  call MPI_Type_extent(mpi_real_kind, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
+  call MPI_Type_get_extent(MPI_INTEGER, integer_lb, integer_extent, ierr);  assert(ierr == MPI_SUCCESS)
+  call MPI_Type_get_extent(mpi_real_kind, real_lb, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
 
   ! Read the donor mesh partition
   call read_node(trim(basename_a) // "_" // trim(nprocs_chr) // "_" // trim(rank_chr) // ".node", dim = 2, positions = positions_a)
--- a/./src/tests/test_parallel_p1_inner_product_3d.F90	2020-10-05 08:38:08.099514278 -0000
+++ b/./src/tests/test_parallel_p1_inner_product_3d.F90	2020-10-05 08:38:44.591516118 -0000
@@ -43,7 +43,8 @@
     & volume_ref = 3.3333333333333331e2_real_kind / 1.0e3_real_kind, &
     & integral_ref = 1.2499999999999989e4_real_kind / 1.0e5_real_kind
 
-  integer :: ierr, integer_extent, nprocs, rank, real_extent
+  integer :: ierr, nprocs, rank
+  integer(kind = MPI_ADDRESS_KIND) :: integer_lb, integer_extent, real_lb, real_extent
  
   integer :: nelements_a, nelements_b, nnodes_a, nnodes_b
   integer, dimension(:), allocatable :: ele_owner_a, ele_owner_b
@@ -96,8 +97,8 @@
   write(nprocs_chr, "(i0)") nprocs
   rank_chr = adjustl(rank_chr)
   nprocs_chr = adjustl(nprocs_chr)
-  call MPI_Type_extent(MPI_INTEGER, integer_extent, ierr);  assert(ierr == MPI_SUCCESS)
-  call MPI_Type_extent(mpi_real_kind, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
+  call MPI_Type_get_extent(MPI_INTEGER, integer_lb, integer_extent, ierr);  assert(ierr == MPI_SUCCESS)
+  call MPI_Type_get_extent(mpi_real_kind, real_lb, real_extent, ierr);  assert(ierr == MPI_SUCCESS)
 
   ! Read the donor mesh partition
   call read_node(trim(basename_a) // "_" // trim(nprocs_chr) // "_" // trim(rank_chr) // ".node", dim = 3, positions = positions_a)
--- a/./doc/manual.tex	2020-10-05 08:41:22.951524101 -0000
+++ b/./doc/manual.tex	2020-10-05 08:41:29.507524432 -0000
@@ -2208,7 +2208,8 @@
 Variables are defined.
 \begin{lstlisting}[language=FORTRAN]
   character(len = int(log10(real(huge(0)))) + 2) :: rank_chr
-  integer :: ierr, rank, real_extent
+  integer :: ierr, rank
+  integer(kind = MPI_ADDRESS_KIND) :: real_lb, real_extent
   
   real, dimension(:, :), allocatable :: positions_a, positions_b
   integer, dimension(:, :), allocatable :: enlist_a, enlist_b
@@ -2232,7 +2233,7 @@
   if(ierr /= MPI_SUCCESS) stop 1
   
   ! Find the extent of a double precision variable
-  call MPI_Type_extent(MPI_DOUBLE_PRECISION, real_extent, ierr)
+  call MPI_Type_get_extent(MPI_DOUBLE_PRECISION, real_lb, real_extent, ierr)
   if(ierr /= MPI_SUCCESS) call MPI_Abort(MPI_COMM_WORLD, MPI_ERR_OTHER, ierr)
 \end{lstlisting}
 
@@ -2490,7 +2491,8 @@
   implicit none
   
   character(len = int(log10(real(huge(0)))) + 2) :: rank_chr
-  integer :: ierr, rank, real_extent
+  integer :: ierr, rank
+  integer(kind = MPI_ADDRESS_KIND) :: real_lb, real_extent
   
   real, dimension(:, :), allocatable :: positions_a, positions_b
   integer, dimension(:, :), allocatable :: enlist_a, enlist_b
@@ -2508,7 +2510,7 @@
   if(ierr /= MPI_SUCCESS) stop 1
   
   ! Find the extent of a double precision variable
-  call MPI_Type_extent(MPI_DOUBLE_PRECISION, real_extent, ierr)
+  call MPI_Type_get_extent(MPI_DOUBLE_PRECISION, real_lb, real_extent, ierr)
   if(ierr /= MPI_SUCCESS) call MPI_Abort(MPI_COMM_WORLD, MPI_ERR_OTHER, ierr)
   
   ! Find the process rank, and convert to a string
